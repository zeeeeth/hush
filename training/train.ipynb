{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea12af0",
   "metadata": {},
   "source": [
    "# GNN Training\n",
    "\n",
    "This notebook trains a **Graph Neural Network (GNN)** to predict next-hour subway ridership across all ~426 MTA station complexes.\n",
    "\n",
    "**How it works:**\n",
    "- The NYC subway network is modeled as a **graph** — stations are nodes, physical track connections are edges\n",
    "- At each hour, the model sees every station's current ridership + time encoding, and predicts what ridership will be one hour from now\n",
    "- The GNN architecture lets information flow between connected stations, so the model can learn patterns like \"when Times Square gets busy, nearby stations get busy too\"\n",
    "\n",
    "**Data pipeline:**\n",
    "1. `preprocess.py` splits yearly CSVs into train/val/test parquet files and computes per-station normalization stats\n",
    "2. This notebook loads those parquet files and trains the model\n",
    "\n",
    "**Split strategy (temporal, no data leakage):**\n",
    "- 2020–2022: 100% train\n",
    "- 2023–2025: Jan–Jun → train, Jul–Sep → validation, Oct–Dec → test\n",
    "\n",
    "Stats (mean/std) are computed **only from training data** to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3516e1",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5baa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "EPOCHS = 15       # Max number of passes through the full training set\n",
    "LR = 1e-4         # Learning rate for Adam optimizer (how big each weight update step is)\n",
    "HIDDEN_DIM = 64   # Size of the hidden layer in the GNN (more = more expressive but slower)\n",
    "PATIENCE = 4      # Stop training if val loss doesn't improve for this many consecutive epochs\n",
    "\n",
    "# --- Paths ---\n",
    "# ROOT points to the project root (one level up from training/)\n",
    "ROOT = os.path.dirname(os.path.abspath(\"\"))  # from training/ directory\n",
    "PROC_DIR = os.path.join(ROOT, \"data\", \"processed\")\n",
    "MODEL_DIR = os.path.join(ROOT, \"models\")\n",
    "EDGES_PATH = os.path.join(PROC_DIR, \"ComplexEdges.csv\")   # which stations are connected\n",
    "CMPLX_PATH = os.path.join(PROC_DIR, \"ComplexNodes.csv\")   # station complex ID → node index mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7ae52",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "The GNN has 3 layers:\n",
    "1. **GCNConv layer 1** — takes the 3 input features per station and outputs 64-dimensional embeddings. Each station's output incorporates info from its neighbors via graph convolution.\n",
    "2. **GCNConv layer 2** — another round of message passing. Now each station has info from stations **2 hops away** (neighbor's neighbors).\n",
    "3. **Linear head** — maps each station's 64-dim embedding down to a single number: the predicted next-hour ridership (in normalized space).\n",
    "\n",
    "Both GCN layers use **ReLU** activation (clips negatives to 0) to introduce non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Layer 1: graph convolution from input features → hidden_dim\n",
    "        # in_dim=3 because each station gets: [ridership_norm, sin(hour), cos(hour)]\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        # Layer 2: another graph convolution, hidden_dim → hidden_dim\n",
    "        # This second layer lets the model see 2-hop neighbors\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        # Output head: maps each station's hidden representation to 1 predicted value\n",
    "        self.mlp = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x shape: (num_nodes, 3) — features for every station\n",
    "        # edge_index shape: (2, num_edges) — pairs of connected station indices\n",
    "        h = torch.relu(self.conv1(x, edge_index))   # (num_nodes, hidden_dim)\n",
    "        h = torch.relu(self.conv2(h, edge_index))   # (num_nodes, hidden_dim)\n",
    "        return self.mlp(h).squeeze()                 # (num_nodes,) — one prediction per station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c6467",
   "metadata": {},
   "source": [
    "## Load Graph Structure\n",
    "\n",
    "The subway network graph needs two things:\n",
    "- **Node mapping** (`cmplx_to_node.csv`): maps MTA's station complex IDs (e.g. 611) to sequential indices (0, 1, 2, ...) that PyTorch needs\n",
    "- **Edges** (`complex_edges.csv`): pairs of connected stations. We add both directions (A→B and B→A) since trains run both ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the station complex ID → node index mapping\n",
    "# e.g. complex_id 611 (Times Sq) → node_id 0\n",
    "cmplx_df = pd.read_csv(CMPLX_PATH)\n",
    "ComplexNodes = dict(zip(cmplx_df[\"complex_id\"], cmplx_df[\"node_id\"]))\n",
    "num_nodes = int(cmplx_df[\"node_id\"].max() + 1)\n",
    "\n",
    "# Load edges (physical track connections between stations)\n",
    "edges_df = pd.read_csv(EDGES_PATH)\n",
    "edge_list = []\n",
    "for _, row in edges_df.iterrows():\n",
    "    s, e = row[\"from_complex_id\"], row[\"to_complex_id\"]\n",
    "    if s in ComplexNodes and e in ComplexNodes:\n",
    "        sn, en = ComplexNodes[s], ComplexNodes[e]\n",
    "        # Add both directions — undirected graph (trains go both ways)\n",
    "        edge_list.append([sn, en])\n",
    "        edge_list.append([en, sn])\n",
    "\n",
    "# PyG expects edge_index as a (2, num_edges) tensor:\n",
    "# row 0 = source nodes, row 1 = destination nodes\n",
    "edge_tensor = torch.tensor(edge_list, dtype=torch.long).T\n",
    "print(f\"Nodes: {num_nodes}, Edges: {edge_tensor.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65df72",
   "metadata": {},
   "source": [
    "## Build Graph Snapshots\n",
    "\n",
    "This is where the self-supervised learning setup happens. For every pair of consecutive hours in the data:\n",
    "\n",
    "- **Input (X)**: a matrix of shape `(num_nodes, 3)` where each station gets 3 features:\n",
    "  - `ridership_norm`: z-score normalized ridership at time $t$ (how far above/below that station's average)\n",
    "  - `sin_hour`: sine encoding of the hour (so 23:00 and 00:00 are close together)\n",
    "  - `cos_hour`: cosine encoding of the hour (sin + cos together uniquely identify each hour)\n",
    "- **Target (y)**: normalized ridership at time $t+1$ for each station\n",
    "\n",
    "Gaps > 1 hour (e.g. missing data) are skipped to avoid training on impossible predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_snapshots(df, num_nodes):\n",
    "    \"\"\"Build (features, targets) pairs from consecutive timestamps.\n",
    "    \n",
    "    Each snapshot is one training example:\n",
    "    - features: what the model sees (ridership + time at hour t)\n",
    "    - targets: what the model must predict (ridership at hour t+1)\n",
    "    \"\"\"\n",
    "    # Group all station data by timestamp for fast lookup\n",
    "    groups = {t: g for t, g in df.groupby(\"transit_timestamp\")}\n",
    "    timestamps = sorted(groups.keys())\n",
    "\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for t0, t1 in zip(timestamps[:-1], timestamps[1:]):\n",
    "        # Skip if there's a gap > 1 hour (missing data, wouldn't be a valid prediction)\n",
    "        if (t1 - t0).total_seconds() > 3600:\n",
    "            continue\n",
    "\n",
    "        g0 = groups[t0]  # all stations at time t\n",
    "        g1 = groups[t1]  # all stations at time t+1\n",
    "\n",
    "        # Initialize tensors for all nodes (stations without data stay at 0)\n",
    "        X = torch.zeros(num_nodes, 3)  # input features\n",
    "        y = torch.zeros(num_nodes)     # target ridership\n",
    "\n",
    "        # Get node indices for stations present at each timestamp\n",
    "        idx0 = torch.tensor(g0[\"node_id\"].values)\n",
    "        idx1 = torch.tensor(g1[\"node_id\"].values)\n",
    "\n",
    "        # Fill in the 3 features for time t:\n",
    "        X[idx0, 0] = torch.tensor(g0[\"ridership_norm\"].values.astype(np.float32))  # normalized ridership\n",
    "        X[idx0, 1] = torch.tensor(g0[\"sin_hour\"].values.astype(np.float32))        # sin(hour)\n",
    "        X[idx0, 2] = torch.tensor(g0[\"cos_hour\"].values.astype(np.float32))        # cos(hour)\n",
    "\n",
    "        # Target: normalized ridership at time t+1\n",
    "        y[idx1] = torch.tensor(g1[\"ridership_norm\"].values.astype(np.float32))\n",
    "\n",
    "        features.append(X)\n",
    "        targets.append(y)\n",
    "\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a17a0",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "The training parquet contains all 2020–2022 data plus Jan–Jun of 2023–2025. Each row has one station's ridership at one timestamp, already normalized and with time encodings computed by `preprocess.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed training data (already has ridership_norm, sin_hour, cos_hour, node_id)\n",
    "train_df = pd.read_parquet(os.path.join(PROC_DIR, \"train.parquet\"))\n",
    "print(f\"Train rows: {len(train_df):,}\")\n",
    "\n",
    "# Convert the flat dataframe into a list of graph snapshots (one per consecutive hour pair)\n",
    "train_features, train_targets = build_snapshots(train_df, num_nodes)\n",
    "print(f\"Train snapshots: {len(train_features)}\")\n",
    "\n",
    "# Free memory — we only need the snapshot tensors from here on\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91814a44",
   "metadata": {},
   "source": [
    "## Load Validation Data\n",
    "\n",
    "The validation set is Jul–Sep of 2023–2025. The model never trains on this — it's used to detect when the model starts overfitting (memorizing training data instead of learning general patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_parquet(os.path.join(PROC_DIR, \"val.parquet\"))\n",
    "print(f\"Val rows: {len(val_df):,}\")\n",
    "\n",
    "val_features, val_targets = build_snapshots(val_df, num_nodes)\n",
    "print(f\"Val snapshots: {len(val_features)}\")\n",
    "del val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccda2cc",
   "metadata": {},
   "source": [
    "## Initialize Model\n",
    "\n",
    "- **Adam optimizer**: adaptive learning rate optimizer — adjusts step size per parameter, works well out of the box\n",
    "- **MSE loss**: Mean Squared Error — penalizes large prediction errors more than small ones (squared), which pushes the model to avoid big mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 3 input features and 64-dim hidden layers\n",
    "model = GNN(in_dim=3, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "# Adam: adaptive moment estimation — adjusts LR per parameter based on gradient history\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# MSE: average of (prediction - truth)² across all stations and snapshots\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "best_model_path = os.path.join(MODEL_DIR, \"model.pt\")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Will save best model to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bcc6af",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "**How early stopping works:**\n",
    "1. After each epoch, we check validation loss (performance on unseen Jul–Sep data)\n",
    "2. If val loss improved → save the model checkpoint (this is the best model so far)\n",
    "3. If val loss didn't improve for `PATIENCE` consecutive epochs → stop training\n",
    "4. This prevents **overfitting** — the model memorizing training data instead of learning real patterns\n",
    "\n",
    "Without early stopping, the model would keep \"improving\" on training data while getting *worse* at predicting unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the best validation loss seen so far (starts at infinity)\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0  # how many epochs since last improvement\n",
    "\n",
    "print(f\"{'Epoch':>6}  {'Train Loss':>11}  {'Val Loss':>11}  {'Status':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # === TRAINING PHASE ===\n",
    "    # model.train() enables dropout/batchnorm training behavior (not used here, but good practice)\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Loop through every snapshot: feed in hour t, predict hour t+1, update weights\n",
    "    for X, y in tqdm(zip(train_features, train_targets), total=len(train_features), desc=f\"Epoch {epoch}\", leave=False):\n",
    "        optimizer.zero_grad()           # clear gradients from previous step\n",
    "        y_hat = model(X, edge_tensor)   # forward pass: predict next-hour ridership for all stations\n",
    "        loss = loss_fn(y_hat, y)        # compute MSE between predictions and actual values\n",
    "        loss.backward()                 # backpropagation: compute gradient of loss w.r.t. each weight\n",
    "        optimizer.step()                # update weights in the direction that reduces loss\n",
    "        total_train_loss += loss.item()\n",
    "    train_loss = total_train_loss / len(train_features)\n",
    "\n",
    "    # === VALIDATION PHASE ===\n",
    "    # model.eval() + no_grad: no weight updates, just measure how well the model generalizes\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # disable gradient tracking (saves memory, faster)\n",
    "        for X, y in zip(val_features, val_targets):\n",
    "            y_hat = model(X, edge_tensor)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            total_val_loss += loss.item()\n",
    "    val_loss = total_val_loss / len(val_features)\n",
    "\n",
    "    # === EARLY STOPPING CHECK ===\n",
    "    if val_loss < best_val_loss:\n",
    "        # New best! Save this checkpoint — it's the best model so far\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        status = \"★ saved\"\n",
    "    else:\n",
    "        # No improvement — increment patience counter\n",
    "        patience_counter += 1\n",
    "        status = f\"wait {patience_counter}/{PATIENCE}\"\n",
    "\n",
    "    print(f\"  {epoch:>4}   {train_loss:>11.6f}  {val_loss:>11.6f}  {status:>10}\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        # Val loss hasn't improved for PATIENCE epochs — the model is probably overfitting\n",
    "        print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {PATIENCE} epochs)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest val loss: {best_val_loss:.6f}\")\n",
    "print(f\"Model saved:   {best_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
