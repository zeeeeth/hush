{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea12af0",
   "metadata": {},
   "source": [
    "# GNN Training\n",
    "\n",
    "This notebook trains a **Graph Neural Network (GNN)** to predict next-hour subway ridership across all ~426 MTA station complexes.\n",
    "\n",
    "**How it works:**\n",
    "- The NYC subway network is modeled as a **graph** — stations are nodes, physical track connections are edges\n",
    "- At each hour, the model sees every station's current ridership + time encoding, and predicts what ridership will be one hour from now\n",
    "- The GNN architecture lets information flow between connected stations, so the model can learn patterns like \"when Times Square gets busy, nearby stations get busy too\"\n",
    "\n",
    "**Data pipeline:**\n",
    "1. `preprocess.py` splits yearly CSVs into train/val/test parquet files and computes per-station normalization stats\n",
    "2. This notebook loads those parquet files and trains the model\n",
    "\n",
    "**Split strategy (temporal, no data leakage):**\n",
    "- 2020–2022: 100% train\n",
    "- 2023–2025: Jan–Jun → train, Jul–Sep → validation, Oct–Dec → test\n",
    "\n",
    "Stats (mean/std) are computed **only from training data** to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49fd7096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\setho\\ichack26\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3516e1",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5baa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "EPOCHS = 15       # Max number of passes through the full training set\n",
    "LR = 1e-4         # Learning rate for Adam optimizer (how big each weight update step is)\n",
    "HIDDEN_DIM = 64   # Size of the hidden layer in the GNN (more = more expressive but slower)\n",
    "PATIENCE = 4      # Stop training if val loss doesn't improve for this many consecutive epochs\n",
    "\n",
    "# --- Paths ---\n",
    "# ROOT points to the project root (one level up from training/)\n",
    "ROOT = os.path.dirname(os.path.abspath(\"\"))  # from training/ directory\n",
    "PROC_DIR = os.path.join(ROOT, \"data\", \"processed\")\n",
    "MODEL_DIR = os.path.join(ROOT, \"new_models\")\n",
    "EDGES_PATH = os.path.join(PROC_DIR, \"ComplexEdges.csv\")   # which stations are connected\n",
    "CMPLX_PATH = os.path.join(PROC_DIR, \"ComplexNodes.csv\")   # station complex ID -> node index mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7ae52",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "The GNN has 3 layers:\n",
    "1. **GCNConv layer 1** — takes the 3 input features per station and outputs 64-dimensional embeddings. Each station's output incorporates info from its neighbors via graph convolution.\n",
    "2. **GCNConv layer 2** — another round of message passing. Now each station has info from stations **2 hops away** (neighbor's neighbors).\n",
    "3. **Linear head** — maps each station's 64-dim embedding down to a single number: the predicted next-hour ridership (in normalized space).\n",
    "\n",
    "Both GCN layers use **ReLU** activation (clips negatives to 0) to introduce non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Layer 1: graph convolution from input features -> hidden_dim\n",
    "        # in_dim=3 because each station gets: [ridership_norm, sin(hour), cos(hour)]\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        # Layer 2: another graph convolution, hidden_dim -> hidden_dim\n",
    "        # This second layer lets the model see 2-hop neighbors\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        # Output head: maps each station's hidden representation to 1 predicted value\n",
    "        self.mlp = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x shape: (num_nodes, 3) — features for every station\n",
    "        # edge_index shape: (2, num_edges) — pairs of connected station indices\n",
    "        h = torch.relu(self.conv1(x, edge_index))   # (num_nodes, hidden_dim)\n",
    "        h = torch.relu(self.conv2(h, edge_index))   # (num_nodes, hidden_dim)\n",
    "        return self.mlp(h).squeeze()                 # (num_nodes,) — one prediction per station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c6467",
   "metadata": {},
   "source": [
    "## Load Graph Structure\n",
    "\n",
    "The subway network graph needs two things:\n",
    "- **Node mapping** (`cmplx_to_node.csv`): maps MTA's station complex IDs (e.g. 611) to sequential indices (0, 1, 2, ...) that PyTorch needs\n",
    "- **Edges** (`complex_edges.csv`): pairs of connected stations. We add both directions (A→B and B→A) since trains run both ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af2fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 426, Edges: 1106\n"
     ]
    }
   ],
   "source": [
    "# Load the station complex ID -> node index mapping\n",
    "# e.g. complex_id 611 (Times Sq) -> node_id 0\n",
    "cmplx_df = pd.read_csv(CMPLX_PATH)\n",
    "ComplexNodes = dict(zip(cmplx_df[\"complex_id\"], cmplx_df[\"node_id\"]))\n",
    "num_nodes = int(cmplx_df[\"node_id\"].max() + 1)\n",
    "\n",
    "# Load edges (physical track connections between stations)\n",
    "edges_df = pd.read_csv(EDGES_PATH)\n",
    "edge_list = []\n",
    "for _, row in edges_df.iterrows():\n",
    "    s, e = row[\"from_complex_id\"], row[\"to_complex_id\"]\n",
    "    if s in ComplexNodes and e in ComplexNodes:\n",
    "        sn, en = ComplexNodes[s], ComplexNodes[e]\n",
    "        # Add both directions — undirected graph (trains go both ways)\n",
    "        edge_list.append([sn, en])\n",
    "        edge_list.append([en, sn])\n",
    "\n",
    "# PyG expects edge_index as a (2, num_edges) tensor:\n",
    "# row 0 = source nodes, row 1 = destination nodes\n",
    "edge_tensor = torch.tensor(edge_list, dtype=torch.long).T\n",
    "print(f\"Nodes: {num_nodes}, Edges: {edge_tensor.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65df72",
   "metadata": {},
   "source": [
    "## Build Graph Snapshots\n",
    "\n",
    "This is where the self-supervised learning setup happens. For every pair of consecutive hours in the data:\n",
    "\n",
    "- **Input (X)**: a matrix of shape `(num_nodes, 3)` where each station gets 3 features:\n",
    "  - `ridership_norm`: z-score normalized ridership at time $t$ (how far above/below that station's average)\n",
    "  - `sin_hour`: sine encoding of the hour (so 23:00 and 00:00 are close together)\n",
    "  - `cos_hour`: cosine encoding of the hour (sin + cos together uniquely identify each hour)\n",
    "- **Target (y)**: normalized ridership at time $t+1$ for each station\n",
    "\n",
    "Gaps > 1 hour (e.g. missing data) are skipped to avoid training on impossible predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2d3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_snapshots(df, num_nodes):\n",
    "    \"\"\"Build (features, targets) pairs from consecutive timestamps.\n",
    "    \n",
    "    Each snapshot is one training example:\n",
    "    - features: what the model sees (ridership + time at hour t)\n",
    "    - targets: what the model must predict (ridership at hour t+1)\n",
    "    \"\"\"\n",
    "    # Group all station data by timestamp for fast lookup\n",
    "    groups = {t: g for t, g in df.groupby(\"transit_timestamp\")}\n",
    "    timestamps = sorted(groups.keys())\n",
    "\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for t0, t1 in zip(timestamps[:-1], timestamps[1:]):\n",
    "        # Skip if there's a gap > 1 hour (missing data, wouldn't be a valid prediction)\n",
    "        if (t1 - t0).total_seconds() > 3600:\n",
    "            continue\n",
    "\n",
    "        g0 = groups[t0]  # all stations at time t\n",
    "        g1 = groups[t1]  # all stations at time t+1\n",
    "\n",
    "        # Initialize tensors for all nodes (stations without data stay at 0)\n",
    "        X = torch.zeros(num_nodes, 3)  # input features\n",
    "        y = torch.zeros(num_nodes)     # target ridership\n",
    "\n",
    "        # Get node indices for stations present at each timestamp\n",
    "        idx0 = torch.tensor(g0[\"node_id\"].values)\n",
    "        idx1 = torch.tensor(g1[\"node_id\"].values)\n",
    "\n",
    "        # Fill in the 3 features for time t:\n",
    "        X[idx0, 0] = torch.tensor(g0[\"ridership_norm\"].values.astype(np.float32))  # normalized ridership\n",
    "        X[idx0, 1] = torch.tensor(g0[\"sin_hour\"].values.astype(np.float32))        # sin(hour)\n",
    "        X[idx0, 2] = torch.tensor(g0[\"cos_hour\"].values.astype(np.float32))        # cos(hour)\n",
    "\n",
    "        # Target: normalized ridership at time t+1\n",
    "        y[idx1] = torch.tensor(g1[\"ridership_norm\"].values.astype(np.float32))\n",
    "\n",
    "        features.append(X)\n",
    "        targets.append(y)\n",
    "\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a17a0",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "The training parquet contains all 2020–2022 data plus Jan–Jun of 2023–2025. Each row has one station's ridership at one timestamp, already normalized and with time encodings computed by `preprocess.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364d52fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\setho\\\\PersonalProjects\\\\hush\\\\data\\\\processed\\\\train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load preprocessed training data (already has ridership_norm, sin_hour, cos_hour, node_id)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROC_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Convert the flat dataframe into a list of graph snapshots (one per consecutive hour pair)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\setho\\ichack26\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\setho\\ichack26\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\setho\\ichack26\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\setho\\ichack26\\venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\setho\\\\PersonalProjects\\\\hush\\\\data\\\\processed\\\\train.parquet'"
     ]
    }
   ],
   "source": [
    "# Load preprocessed training data (already has ridership_norm, sin_hour, cos_hour, node_id)\n",
    "train_df = pd.read_parquet(os.path.join(PROC_DIR, \"train.parquet\"))\n",
    "print(f\"Train rows: {len(train_df):,}\")\n",
    "\n",
    "# Convert the flat dataframe into a list of graph snapshots (one per consecutive hour pair)\n",
    "train_features, train_targets = build_snapshots(train_df, num_nodes)\n",
    "print(f\"Train snapshots: {len(train_features)}\")\n",
    "\n",
    "# Free memory — we only need the snapshot tensors from here on\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91814a44",
   "metadata": {},
   "source": [
    "## Load Validation Data\n",
    "\n",
    "The validation set is Jul–Sep of 2023–2025. The model never trains on this — it's used to detect when the model starts overfitting (memorizing training data instead of learning general patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf2af59",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\setho\\\\PersonalProjects\\\\hush\\\\data\\\\processed\\\\val.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m val_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROC_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m val_features, val_targets = build_snapshots(val_df, num_nodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\setho\\ichack26\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\setho\\ichack26\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\setho\\ichack26\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\setho\\ichack26\\venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\setho\\\\PersonalProjects\\\\hush\\\\data\\\\processed\\\\val.parquet'"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_parquet(os.path.join(PROC_DIR, \"val.parquet\"))\n",
    "print(f\"Val rows: {len(val_df):,}\")\n",
    "\n",
    "val_features, val_targets = build_snapshots(val_df, num_nodes)\n",
    "print(f\"Val snapshots: {len(val_features)}\")\n",
    "del val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccda2cc",
   "metadata": {},
   "source": [
    "## Initialize Model\n",
    "\n",
    "- **Adam optimizer**: adaptive learning rate optimizer — adjusts step size per parameter, works well out of the box\n",
    "- **MSE loss**: Mean Squared Error — penalizes large prediction errors more than small ones (squared), which pushes the model to avoid big mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57df791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 4,481\n",
      "Will save best model to: c:\\Users\\setho\\PersonalProjects\\hush\\new_models\\model.pt\n"
     ]
    }
   ],
   "source": [
    "# Create the model with 3 input features and 64-dim hidden layers\n",
    "model = GNN(in_dim=3, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "# Adam: adaptive moment estimation — adjusts LR per parameter based on gradient history\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# MSE: average of (prediction - truth)² across all stations and snapshots\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "best_model_path = os.path.join(MODEL_DIR, \"model.pt\")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Will save best model to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bcc6af",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "**How early stopping works:**\n",
    "1. After each epoch, we check validation loss (performance on unseen Jul–Sep data)\n",
    "2. If val loss improved → save the model checkpoint (this is the best model so far)\n",
    "3. If val loss didn't improve for `PATIENCE` consecutive epochs → stop training\n",
    "4. This prevents **overfitting** — the model memorizing training data instead of learning real patterns\n",
    "\n",
    "Without early stopping, the model would keep \"improving\" on training data while getting *worse* at predicting unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa02ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch   Train Loss     Val Loss      Status\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mTrain Loss\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>11\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mVal Loss\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>11\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mStatus\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[43mEPOCHS\u001b[49m + \u001b[32m1\u001b[39m):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# === TRAINING PHASE ===\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# model.train() enables dropout/batchnorm training behavior (not used here, but good practice)\u001b[39;00m\n\u001b[32m     11\u001b[39m     model.train()\n\u001b[32m     12\u001b[39m     total_train_loss = \u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "# Track the best validation loss seen so far (starts at infinity)\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0  # how many epochs since last improvement\n",
    "\n",
    "print(f\"{'Epoch':>6}  {'Train Loss':>11}  {'Val Loss':>11}  {'Status':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # === TRAINING PHASE ===\n",
    "    # model.train() enables dropout/batchnorm training behavior (not used here, but good practice)\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Loop through every snapshot: feed in hour t, predict hour t+1, update weights\n",
    "    for X, y in tqdm(zip(train_features, train_targets), total=len(train_features), desc=f\"Epoch {epoch}\", leave=False):\n",
    "        optimizer.zero_grad()           # clear gradients from previous step\n",
    "        y_hat = model(X, edge_tensor)   # forward pass: predict next-hour ridership for all stations\n",
    "        loss = loss_fn(y_hat, y)        # compute MSE between predictions and actual values\n",
    "        loss.backward()                 # backpropagation: compute gradient of loss w.r.t. each weight\n",
    "        optimizer.step()                # update weights in the direction that reduces loss\n",
    "        total_train_loss += loss.item()\n",
    "    train_loss = total_train_loss / len(train_features)\n",
    "\n",
    "    # === VALIDATION PHASE ===\n",
    "    # model.eval() + no_grad: no weight updates, just measure how well the model generalizes\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # disable gradient tracking (saves memory, faster)\n",
    "        for X, y in zip(val_features, val_targets):\n",
    "            y_hat = model(X, edge_tensor)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            total_val_loss += loss.item()\n",
    "    val_loss = total_val_loss / len(val_features)\n",
    "\n",
    "    # === EARLY STOPPING CHECK ===\n",
    "    if val_loss < best_val_loss:\n",
    "        # New best! Save this checkpoint — it's the best model so far\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        status = \"★ saved\"\n",
    "    else:\n",
    "        # No improvement — increment patience counter\n",
    "        patience_counter += 1\n",
    "        status = f\"wait {patience_counter}/{PATIENCE}\"\n",
    "\n",
    "    print(f\"  {epoch:>4}   {train_loss:>11.6f}  {val_loss:>11.6f}  {status:>10}\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        # Val loss hasn't improved for PATIENCE epochs — the model is probably overfitting\n",
    "        print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {PATIENCE} epochs)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest val loss: {best_val_loss:.6f}\")\n",
    "print(f\"Model saved:   {best_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
