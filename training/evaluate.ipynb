{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e748ba",
   "metadata": {},
   "source": [
    "# GNN Evaluation\n",
    "\n",
    "This notebook evaluates the trained GNN model on **unseen data** (test or validation split) and reports detailed metrics.\n",
    "\n",
    "**What this does:**\n",
    "- Loads the best model checkpoint from training\n",
    "- Runs it on the test (or val) set: for each hour, predicts next-hour ridership for all stations\n",
    "- Compares predictions to ground truth, both in normalized and real tap-in space\n",
    "- Reports overall error, error by hour, and per-station breakdown (so you can see which stations are hardest/easiest to predict)\n",
    "\n",
    "**Why this matters:**\n",
    "- Shows how well the model generalizes to new time periods (not just memorizing training data)\n",
    "- Lets you spot patterns in model errors (e.g. always underpredicts at Times Sq at 8am)\n",
    "- Per-station and per-hour breakdowns help debug model weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d6bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166dc5f8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation settings ---\n",
    "SPLIT = \"test\"      # Which split to evaluate: \"test\" (default) or \"val\"\n",
    "HIDDEN_DIM = 64     # Must match the model you trained\n",
    "\n",
    "# --- Paths ---\n",
    "# ROOT points to the project root (one level up from training/)\n",
    "ROOT = os.path.dirname(os.path.abspath(\"\"))\n",
    "PROC_DIR = os.path.join(ROOT, \"data\", \"processed\")\n",
    "MODEL_PATH = os.path.join(ROOT, \"models\", \"model.pt\")\n",
    "STATS_PATH = os.path.join(PROC_DIR, \"stats.csv\")\n",
    "CMPLX_PATH = os.path.join(PROC_DIR, \"ComplexNodes.csv\")\n",
    "EDGES_PATH = os.path.join(PROC_DIR, \"ComplexEdges.csv\")\n",
    "\n",
    "# The test split is Oct–Dec 2023–2025 (never seen during training/validation)\n",
    "# The val split is Jul–Sep 2023–2025 (used for early stopping, not for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f730f",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "The model architecture must **exactly match** what was used in training:\n",
    "- 2 × GCNConv layers (each with ReLU)\n",
    "- Linear regression head\n",
    "\n",
    "If you change the architecture or hidden size, you must retrain and re-save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782138d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Layer 1: graph convolution from input features → hidden_dim\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        # Layer 2: another graph convolution, hidden_dim → hidden_dim\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        # Output head: maps each station's hidden representation to 1 predicted value\n",
    "        self.mlp = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x shape: (num_nodes, 3) — features for every station\n",
    "        # edge_index shape: (2, num_edges) — pairs of connected station indices\n",
    "        h = torch.relu(self.conv1(x, edge_index))\n",
    "        h = torch.relu(self.conv2(h, edge_index))\n",
    "        return self.mlp(h).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8e07d",
   "metadata": {},
   "source": [
    "## Load Graph Structure & Stats\n",
    "\n",
    "- **Node mapping**: maps MTA station complex IDs to sequential node indices (needed for PyTorch)\n",
    "- **Edges**: pairs of connected stations (bidirectional)\n",
    "- **Stats**: per-station mean and std, used to denormalize predictions back to real tap-in counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb205c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node mapping: complex_id → node_id (0, 1, 2, ...)\n",
    "cmplx_df = pd.read_csv(CMPLX_PATH)\n",
    "ComplexNodes = dict(zip(cmplx_df[\"complex_id\"], cmplx_df[\"node_id\"]))\n",
    "node_to_cmplx = dict(zip(cmplx_df[\"node_id\"], cmplx_df[\"complex_id\"]))\n",
    "num_nodes = int(cmplx_df[\"node_id\"].max() + 1)\n",
    "\n",
    "# Load edges (track connections between stations)\n",
    "edges_df = pd.read_csv(EDGES_PATH)\n",
    "edge_list = []\n",
    "for _, row in edges_df.iterrows():\n",
    "    s, e = row[\"from_complex_id\"], row[\"to_complex_id\"]\n",
    "    if s in ComplexNodes and e in ComplexNodes:\n",
    "        sn, en = ComplexNodes[s], ComplexNodes[e]\n",
    "        # Add both directions (undirected graph)\n",
    "        edge_list.append([sn, en])\n",
    "        edge_list.append([en, sn])\n",
    "edge_tensor = torch.tensor(edge_list, dtype=torch.long).T\n",
    "\n",
    "# Load per-station normalization stats (mean/std from training set)\n",
    "stats = pd.read_csv(STATS_PATH)\n",
    "stn_mean = dict(zip(stats[\"station_complex_id\"], stats[\"mean\"]))\n",
    "stn_std = dict(zip(stats[\"station_complex_id\"], stats[\"std\"]))\n",
    "\n",
    "print(f\"Nodes: {num_nodes}, Edges: {edge_tensor.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dacdfd",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Loads the best model checkpoint from training. The architecture and hidden size must match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adcd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and load trained weights\n",
    "model = GNN(in_dim=3, hidden_dim=HIDDEN_DIM)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=\"cpu\"))\n",
    "model.eval()  # disables dropout/batchnorm (not used here, but good practice)\n",
    "print(f\"Loaded model from {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08deae4c",
   "metadata": {},
   "source": [
    "## Load Split Data & Build Snapshots\n",
    "\n",
    "Loads the test or validation split (parquet file). Each row is one station's ridership at one timestamp, already normalized and with time encodings.\n",
    "\n",
    "We then build graph snapshots: for every pair of consecutive hours, the model sees hour $t$ and must predict hour $t+1$ for all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1276c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the split (test or val) as a dataframe\n",
    "split_path = os.path.join(PROC_DIR, f\"{SPLIT}.parquet\")\n",
    "df = pd.read_parquet(split_path)\n",
    "print(f\"{SPLIT.upper()} set: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['transit_timestamp'].min()} → {df['transit_timestamp'].max()}\")\n",
    "\n",
    "# Build graph snapshots: for each consecutive hour, create (features, targets, raw_targets, hour)\n",
    "groups = {t: g for t, g in df.groupby(\"transit_timestamp\")}\n",
    "timestamps = sorted(groups.keys())\n",
    "\n",
    "features, targets, raw_targets, hours = [], [], [], []\n",
    "\n",
    "for t0, t1 in tqdm(zip(timestamps[:-1], timestamps[1:]), total=len(timestamps)-1, desc=\"Building snapshots\"):\n",
    "    # Skip if there's a gap > 1 hour (missing data)\n",
    "    if (t1 - t0).total_seconds() > 3600:\n",
    "        continue\n",
    "\n",
    "    g0 = groups[t0]\n",
    "    g1 = groups[t1]\n",
    "\n",
    "    X = torch.zeros(num_nodes, 3)\n",
    "    y = torch.zeros(num_nodes)\n",
    "    y_raw = torch.zeros(num_nodes)\n",
    "\n",
    "    idx0 = torch.tensor(g0[\"node_id\"].values)\n",
    "    idx1 = torch.tensor(g1[\"node_id\"].values)\n",
    "\n",
    "    X[idx0, 0] = torch.tensor(g0[\"ridership_norm\"].values.astype(np.float32))\n",
    "    X[idx0, 1] = torch.tensor(g0[\"sin_hour\"].values.astype(np.float32))\n",
    "    X[idx0, 2] = torch.tensor(g0[\"cos_hour\"].values.astype(np.float32))\n",
    "\n",
    "    y[idx1] = torch.tensor(g1[\"ridership_norm\"].values.astype(np.float32))\n",
    "    y_raw[idx1] = torch.tensor(g1[\"ridership\"].values.astype(np.float32))\n",
    "\n",
    "    features.append(X)\n",
    "    targets.append(y)\n",
    "    raw_targets.append(y_raw)\n",
    "    hours.append(pd.Timestamp(t1).hour)\n",
    "\n",
    "print(f\"Snapshots: {len(features)}\")\n",
    "del df, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: list of (num_nodes, 3) tensors (input for each hour)\n",
    "# targets: list of (num_nodes,) tensors (normalized ground truth for next hour)\n",
    "# raw_targets: list of (num_nodes,) tensors (real tap-in counts for next hour)\n",
    "# hours: list of int (hour of day for each snapshot)\n",
    "# These are used for evaluation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53740ccf",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "For each snapshot (hour), the model predicts next-hour ridership for all stations. We collect both normalized and real (denormalized) predictions for metric calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each snapshot, run the model and collect predictions and ground truth\n",
    "# We store both normalized and denormalized (real tap-in) values for metrics\n",
    "all_pred_norm, all_true_norm = [], []\n",
    "all_pred_raw, all_true_raw = [], []\n",
    "all_hours, all_stations = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y_norm, y_raw, hour in tqdm(\n",
    "        zip(features, targets, raw_targets, hours),\n",
    "        total=len(features),\n",
    "        desc=\"Evaluating\",\n",
    "    ):\n",
    "        y_hat_norm = model(X, edge_tensor)  # predict next-hour ridership (normalized)\n",
    "\n",
    "        for node_id in range(num_nodes):\n",
    "            if node_id not in node_to_cmplx:\n",
    "                continue\n",
    "\n",
    "            cmplx_id = node_to_cmplx[node_id]\n",
    "            true_norm = y_norm[node_id].item()\n",
    "            pred_norm = y_hat_norm[node_id].item()\n",
    "            true_raw_val = y_raw[node_id].item()\n",
    "\n",
    "            # Denormalize prediction: pred_raw = pred_norm * std + mean\n",
    "            mean = stn_mean.get(cmplx_id, 0)\n",
    "            std = stn_std.get(cmplx_id, 1)\n",
    "            pred_raw = pred_norm * std + mean\n",
    "\n",
    "            all_pred_norm.append(pred_norm)\n",
    "            all_true_norm.append(true_norm)\n",
    "            all_pred_raw.append(max(0, pred_raw))  # don't allow negative tap-ins\n",
    "            all_true_raw.append(true_raw_val)\n",
    "            all_hours.append(hour)\n",
    "            all_stations.append(cmplx_id)\n",
    "\n",
    "pred_norm = np.array(all_pred_norm)\n",
    "true_norm = np.array(all_true_norm)\n",
    "pred_raw = np.array(all_pred_raw)\n",
    "true_raw = np.array(all_true_raw)\n",
    "hours_arr = np.array(all_hours)\n",
    "stations_arr = np.array(all_stations)\n",
    "\n",
    "print(f\"Total predictions: {len(pred_raw):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60714eb2",
   "metadata": {},
   "source": [
    "## Overall Metrics\n",
    "\n",
    "Calculates and prints:\n",
    "- **MSE/MAE in normalized space** (z-score units)\n",
    "- **MSE/MAE/RMSE in real tap-in space** (actual number of people)\n",
    "- **Median absolute error** (robust to outliers)\n",
    "- **R² score** (how much variance is explained by the model)\n",
    "\n",
    "This gives a sense of both relative and absolute model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee68573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalized space (z-score units) ---\n",
    "mse_norm = np.mean((pred_norm - true_norm) ** 2)\n",
    "mae_norm = np.mean(np.abs(pred_norm - true_norm))\n",
    "\n",
    "# --- Real tap-in space (actual people) ---\n",
    "mse_raw = np.mean((pred_raw - true_raw) ** 2)\n",
    "mae_raw = np.mean(np.abs(pred_raw - true_raw))\n",
    "rmse_raw = np.sqrt(mse_raw)\n",
    "\n",
    "# --- R² score ---\n",
    "ss_res = np.sum((true_raw - pred_raw) ** 2)\n",
    "ss_tot = np.sum((true_raw - np.mean(true_raw)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "# --- Median absolute error ---\n",
    "median_ae = np.median(np.abs(pred_raw - true_raw))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"OVERALL METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Normalized space:\")\n",
    "print(f\"    MSE  = {mse_norm:.4f}\")\n",
    "print(f\"    MAE  = {mae_norm:.4f}\")\n",
    "print(f\"\\n  Real tap-in space:\")\n",
    "print(f\"    MSE   = {mse_raw:.2f}\")\n",
    "print(f\"    RMSE  = {rmse_raw:.2f}\")\n",
    "print(f\"    MAE   = {mae_raw:.2f} tap-ins\")\n",
    "print(f\"    MedAE = {median_ae:.2f} tap-ins\")\n",
    "print(f\"    R²    = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2f918",
   "metadata": {},
   "source": [
    "## Error by Hour of Day\n",
    "\n",
    "Shows how model error varies by time of day (e.g. does it struggle more at rush hour?).\n",
    "\n",
    "For each hour, prints MAE (in tap-ins) and a bar chart for quick visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a85dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Hour':>6}  {'MAE':>8}  {'Count':>8}  Bar\")\n",
    "print(\"-\" * 45)\n",
    "for h in range(24):\n",
    "    mask = hours_arr == h\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    h_mae = np.mean(np.abs(pred_raw[mask] - true_raw[mask]))\n",
    "    h_count = mask.sum()\n",
    "    bar = \"█\" * int(h_mae / 5)\n",
    "    print(f\"  {h:02d}:00  {h_mae:>8.2f}  {h_count:>8}  {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d597f5c",
   "metadata": {},
   "source": [
    "## Per-Station Breakdown\n",
    "\n",
    "For each station, computes:\n",
    "- MAE (mean absolute error in tap-ins)\n",
    "- Average true ridership\n",
    "- MAPE (mean absolute percentage error)\n",
    "- Number of predictions\n",
    "\n",
    "Prints the 10 worst and 10 best stations by MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c719b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_errors = {}\n",
    "for cmplx_id in np.unique(stations_arr):\n",
    "    mask = stations_arr == cmplx_id\n",
    "    if mask.sum() < 5:\n",
    "        continue\n",
    "    s_mae = np.mean(np.abs(pred_raw[mask] - true_raw[mask]))\n",
    "    s_avg_ridership = np.mean(true_raw[mask])\n",
    "    station_errors[cmplx_id] = {\n",
    "        \"mae\": s_mae,\n",
    "        \"avg_ridership\": s_avg_ridership,\n",
    "        \"mape\": (s_mae / (s_avg_ridership + 1e-6)) * 100,\n",
    "        \"n\": int(mask.sum()),\n",
    "    }\n",
    "\n",
    "sorted_stations = sorted(station_errors.items(), key=lambda x: x[1][\"mae\"], reverse=True)\n",
    "\n",
    "print(\"Top 10 WORST stations (highest MAE):\")\n",
    "print(f\"  {'Station':>10}  {'MAE':>8}  {'Avg Ridership':>14}  {'MAPE%':>7}  {'n':>6}\")\n",
    "print(f\"  {'-'*10}  {'-'*8}  {'-'*14}  {'-'*7}  {'-'*6}\")\n",
    "for cmplx_id, err in sorted_stations[:10]:\n",
    "    print(f\"  {cmplx_id:>10}  {err['mae']:>8.2f}  {err['avg_ridership']:>14.2f}  {err['mape']:>6.1f}%  {err['n']:>6}\")\n",
    "\n",
    "print(\"Top 10 BEST stations (lowest MAE):\")\n",
    "print(f\"  {'Station':>10}  {'MAE':>8}  {'Avg Ridership':>14}  {'MAPE%':>7}  {'n':>6}\")\n",
    "print(f\"  {'-'*10}  {'-'*8}  {'-'*14}  {'-'*7}  {'-'*6}\")\n",
    "for cmplx_id, err in sorted_stations[-10:]:\n",
    "    print(f\"  {cmplx_id:>10}  {err['mae']:>8.2f}  {err['avg_ridership']:>14.2f}  {err['mape']:>6.1f}%  {err['n']:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 WORST stations (highest MAE):\")\n",
    "print(f\"  {'Station':>10}  {'MAE':>8}  {'Avg Ridership':>14}  {'MAPE%':>7}  {'n':>6}\")\n",
    "print(f\"  {'-'*10}  {'-'*8}  {'-'*14}  {'-'*7}  {'-'*6}\")\n",
    "for cmplx_id, err in sorted_stations[:10]:\n",
    "    print(f\"  {cmplx_id:>10}  {err['mae']:>8.2f}  {err['avg_ridership']:>14.2f}  {err['mape']:>6.1f}%  {err['n']:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 BEST stations (lowest MAE):\")\n",
    "print(f\"  {'Station':>10}  {'MAE':>8}  {'Avg Ridership':>14}  {'MAPE%':>7}  {'n':>6}\")\n",
    "print(f\"  {'-'*10}  {'-'*8}  {'-'*14}  {'-'*7}  {'-'*6}\")\n",
    "for cmplx_id, err in sorted_stations[-10:]:\n",
    "    print(f\"  {cmplx_id:>10}  {err['mae']:>8.2f}  {err['avg_ridership']:>14.2f}  {err['mape']:>6.1f}%  {err['n']:>6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360f996",
   "metadata": {},
   "source": [
    "## MAPE Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapes = [v[\"mape\"] for v in station_errors.values()]\n",
    "print(f\"MAPE across stations:\")\n",
    "print(f\"  Median = {np.median(mapes):.1f}%\")\n",
    "print(f\"  Mean   = {np.mean(mapes):.1f}%\")\n",
    "print(f\"  25th   = {np.percentile(mapes, 25):.1f}%\")\n",
    "print(f\"  75th   = {np.percentile(mapes, 75):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c784eb",
   "metadata": {},
   "source": [
    "## MAPE Distribution\n",
    "\n",
    "Shows the distribution of mean absolute percentage error (MAPE) across all stations. Useful for understanding typical vs. worst-case error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e62c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapes = [v[\"mape\"] for v in station_errors.values()]\n",
    "print(f\"MAPE across stations:\")\n",
    "print(f\"  Median = {np.median(mapes):.1f}%\")\n",
    "print(f\"  Mean   = {np.mean(mapes):.1f}%\")\n",
    "print(f\"  25th   = {np.percentile(mapes, 25):.1f}%\")\n",
    "print(f\"  75th   = {np.percentile(mapes, 75):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
