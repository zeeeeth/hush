{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e748ba",
   "metadata": {},
   "source": [
    "# GNN Evaluation\n",
    "\n",
    "This notebook evaluates the trained GNN model on unseen test data.\n",
    "- Loads the best model checkpoint from training\n",
    "- Runs it on the test set: for each hour, predicts next-hour ridership for all stations\n",
    "- Compares predictions to ground truth, both in normalized and real space\n",
    "- Reports overall error, error by hour, and per-station breakdown\n",
    "- See which stations are easiest/hardest to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e42d6bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166dc5f8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad4b1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = \"test\"\n",
    "HIDDEN_DIM = 64\n",
    "\n",
    "# Paths\n",
    "ROOT = os.path.dirname(os.path.abspath(\"\"))\n",
    "PROC_DIR = os.path.join(ROOT, \"data\", \"processed\")\n",
    "MODEL_PATH = os.path.join(ROOT, \"models\", \"model.pt\")\n",
    "STATS_PATH = os.path.join(PROC_DIR, \"stats.csv\")\n",
    "CMPLX_PATH = os.path.join(PROC_DIR, \"ComplexNodes.csv\")\n",
    "EDGES_PATH = os.path.join(PROC_DIR, \"ComplexEdges.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f730f",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Match the model architecture used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "782138d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirSAGEEmbRes(nn.Module):\n",
    "    def __init__(self, num_nodes: int, in_dim: int, hidden_dim: int, emb_dim: int = 16):\n",
    "        super().__init__()\n",
    "        self.node_emb = nn.Embedding(num_nodes, emb_dim)\n",
    "        d0 = in_dim + emb_dim\n",
    "\n",
    "        self.in1 = SAGEConv(d0, hidden_dim)\n",
    "        self.in2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.out1 = SAGEConv(d0, hidden_dim)\n",
    "        self.out2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.lin = nn.Linear(2 * hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_in, edge_out):\n",
    "        node_ids = torch.arange(x.size(0), device=x.device)\n",
    "        x = torch.cat([x, self.node_emb(node_ids)], dim=1)\n",
    "\n",
    "        h_in1 = torch.relu(self.in1(x, edge_in))\n",
    "        h_in2 = torch.relu(self.in2(h_in1, edge_in))\n",
    "        h_in  = h_in2 + h_in1\n",
    "\n",
    "        h_out1 = torch.relu(self.out1(x, edge_out))\n",
    "        h_out2 = torch.relu(self.out2(h_out1, edge_out))\n",
    "        h_out  = h_out2 + h_out1\n",
    "\n",
    "        h = torch.cat([h_in, h_out], dim=-1)\n",
    "        return self.lin(h).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8e07d",
   "metadata": {},
   "source": [
    "## Load Graph Structure & Stats\n",
    "\n",
    "- **Node mapping**: maps station complex IDs to node indices\n",
    "- **Edges**: pairs of connected stations\n",
    "- **Stats**: per-station mean and std, used to denormalize predictions back to real counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5cb205c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 424, Edge_in: 976, Edge_out: 976\n"
     ]
    }
   ],
   "source": [
    "# Load node mapping\n",
    "cmplx_df = pd.read_csv(CMPLX_PATH)\n",
    "ComplexNodes = dict(zip(cmplx_df[\"complex_id\"], cmplx_df[\"node_id\"]))\n",
    "node_to_cmplx = dict(zip(cmplx_df[\"node_id\"], cmplx_df[\"complex_id\"]))\n",
    "\n",
    "num_nodes = len(ComplexNodes)\n",
    "\n",
    "# Load edges\n",
    "edges_df = pd.read_csv(EDGES_PATH)\n",
    "\n",
    "edge_in_list = []   # from -> to\n",
    "edge_out_list = []  # to -> from\n",
    "\n",
    "for _, row in edges_df.iterrows():\n",
    "    s, e = row[\"from_complex_id\"], row[\"to_complex_id\"]\n",
    "    if s in ComplexNodes and e in ComplexNodes:\n",
    "        u, v = ComplexNodes[s], ComplexNodes[e]\n",
    "        edge_in_list.append([u, v])\n",
    "        edge_out_list.append([v, u])\n",
    "\n",
    "# Add self-loops\n",
    "for i in range(num_nodes):\n",
    "    edge_in_list.append([i, i])\n",
    "    edge_out_list.append([i, i])\n",
    "\n",
    "edge_in = torch.tensor(edge_in_list, dtype=torch.long).T\n",
    "edge_out = torch.tensor(edge_out_list, dtype=torch.long).T\n",
    "\n",
    "# Load per-station normalization stats\n",
    "stats = pd.read_csv(STATS_PATH)\n",
    "stn_mean = dict(zip(stats[\"station_complex_id\"], stats[\"mean\"]))\n",
    "stn_std = dict(zip(stats[\"station_complex_id\"], stats[\"std\"]))\n",
    "\n",
    "print(f\"Nodes: {num_nodes}, Edge_in: {edge_in.shape[1]}, Edge_out: {edge_out.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dacdfd",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Loads the best model checkpoint from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7adcd498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from c:\\Users\\setho\\PersonalProjects\\hush\\models\\model.pt\n"
     ]
    }
   ],
   "source": [
    "model = DirSAGEEmbRes(num_nodes=num_nodes, in_dim=5, hidden_dim=HIDDEN_DIM, emb_dim=16)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "print(f\"Loaded model from {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08deae4c",
   "metadata": {},
   "source": [
    "## Load Split Data & Build Snapshots\n",
    "\n",
    "Loads the test file. Each row is one station's normalised ridership at one timestamp, with time encodings.\n",
    "\n",
    "Build graph snapshots: for every pair of consecutive hours, the model sees hour $t$ and must predict hour $t+1$ for all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb1276c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST set: 3,649,611 rows\n",
      "Date range: 2024-01-01 00:00:00 -> 2024-12-31 23:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building snapshots: 100%|██████████| 8782/8782 [00:07<00:00, 1150.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshots: 8781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load test split\n",
    "split_path = os.path.join(PROC_DIR, f\"{SPLIT}.parquet\")\n",
    "df = pd.read_parquet(split_path)\n",
    "print(f\"{SPLIT.upper()} set: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['transit_timestamp'].min()} -> {df['transit_timestamp'].max()}\")\n",
    "\n",
    "# timestamp -> dataframe containing all stations observed at that timestamp\n",
    "groups = {t: g for t, g in df.groupby(\"transit_timestamp\")}\n",
    "timestamps = sorted(groups.keys())\n",
    "\n",
    "\"\"\"\n",
    "Build one entry per snapshot:\n",
    "- features   : X at time t0\n",
    "- targets    : y at time t1 (normalised ridership)\n",
    "- raw_targets: y_raw at time t1 (actual ridership)\n",
    "- masks      : m (boolean mask), which nodes actually have a target at t1\n",
    "- hours      : hour of day for t1\n",
    "\"\"\"\n",
    "features, targets, raw_targets, masks, hours = [], [], [], [], []\n",
    "\n",
    "for t0, t1 in tqdm(zip(timestamps[:-1], timestamps[1:]), total=len(timestamps)-1, desc=\"Building snapshots\"):\n",
    "    if (t1 - t0).total_seconds() > 3600:\n",
    "        continue\n",
    "    \n",
    "    # Rows for time t0 and t1\n",
    "    g0 = groups[t0]\n",
    "    g1 = groups[t1]\n",
    "\n",
    "    # Allocate dense, node-aligned tensors\n",
    "    X = torch.zeros(num_nodes, 5, dtype=torch.float32)\n",
    "    y = torch.zeros(num_nodes, dtype=torch.float32)\n",
    "    y_raw = torch.zeros(num_nodes, dtype=torch.float32)\n",
    "    m = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Node indices at each timestamp\n",
    "    idx0 = torch.tensor(g0[\"node_id\"].values, dtype=torch.long)\n",
    "    idx1 = torch.tensor(g1[\"node_id\"].values, dtype=torch.long)\n",
    "\n",
    "    # For each node at time t0, write its features to the correct row\n",
    "    X[idx0, 0] = torch.tensor(g0[\"ridership_norm\"].values, dtype=torch.float32)\n",
    "    X[idx0, 1] = torch.tensor(g0[\"sin_hour\"].values, dtype=torch.float32)\n",
    "    X[idx0, 2] = torch.tensor(g0[\"cos_hour\"].values, dtype=torch.float32)\n",
    "    X[idx0, 3] = torch.tensor(g0[\"sin_dow\"].values, dtype=torch.float32)\n",
    "    X[idx0, 4] = torch.tensor(g0[\"cos_dow\"].values, dtype=torch.float32)\n",
    "\n",
    "    # Fill targets for nodes present at t1, build mask, avoid scoring on fake zeros\n",
    "    y[idx1] = torch.tensor(g1[\"ridership_norm\"].values, dtype=torch.float32)\n",
    "    y_raw[idx1] = torch.tensor(g1[\"ridership\"].values, dtype=torch.float32)\n",
    "    m[idx1] = True\n",
    "\n",
    "    # Store snapshot\n",
    "    features.append(X)\n",
    "    targets.append(y)\n",
    "    raw_targets.append(y_raw)\n",
    "    masks.append(m)\n",
    "    hours.append(pd.Timestamp(t1).hour)\n",
    "\n",
    "print(f\"Snapshots: {len(features)}\")\n",
    "del df, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b097e0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 8781/8781 [01:19<00:00, 110.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions (masked): 3,648,795\n"
     ]
    }
   ],
   "source": [
    "# For each snapshot, run the model, collect predictions and ground truth\n",
    "all_pred_norm, all_true_norm = [], [] # Normalised round truth \n",
    "all_pred_raw, all_true_raw = [], []   # Raw ground truth\n",
    "all_hours, all_stations = [], []      # Metadata for slicing results by hour/station\n",
    "\n",
    "# Disable gradient calculation to avoid accidental backpropagation\n",
    "with torch.no_grad():\n",
    "    # Node features, normalised targets, raw targets, mask (which nodes have a target), hour\n",
    "    for X, y_norm, y_raw, m, hour in tqdm(\n",
    "        zip(features, targets, raw_targets, masks, hours),\n",
    "        total=len(features),\n",
    "        desc=\"Evaluating\",\n",
    "    ):\n",
    "        # y          -> ground truth\n",
    "        # y_hat      -> estimate of y made by model\n",
    "        # y_hat_norm -> normalised estimate of y made by model \n",
    "        # Directed dual-pass model\n",
    "        y_hat_norm = model(X, edge_in, edge_out)\n",
    "\n",
    "        idx = torch.where(m)[0]  # Only score nodes with a valid target at t+1\n",
    "\n",
    "        for node_id in idx.tolist():\n",
    "            # Ignore nodes that have no real complex mapping\n",
    "            if node_id not in node_to_cmplx:\n",
    "                continue\n",
    "            \n",
    "            # Ground truth and prediction for this node\n",
    "            cmplx_id = node_to_cmplx[node_id]\n",
    "            true_norm = y_norm[node_id].item()\n",
    "            pred_norm = y_hat_norm[node_id].item()\n",
    "            true_raw_val = y_raw[node_id].item()\n",
    "\n",
    "            # Un-normalise prediction back to raw units\n",
    "            mean = stn_mean.get(cmplx_id, 0)\n",
    "            std = stn_std.get(cmplx_id, 1)\n",
    "            pred_raw = pred_norm * std + mean\n",
    "\n",
    "            # Store results and clamp negative predictions\n",
    "            all_pred_norm.append(pred_norm)\n",
    "            all_true_norm.append(true_norm)\n",
    "            all_pred_raw.append(max(0, pred_raw))\n",
    "            all_true_raw.append(true_raw_val)\n",
    "            all_hours.append(hour)\n",
    "            all_stations.append(cmplx_id)\n",
    "\n",
    "\n",
    "pred_norm = np.array(all_pred_norm)\n",
    "true_norm = np.array(all_true_norm)\n",
    "pred_raw = np.array(all_pred_raw)\n",
    "true_raw = np.array(all_true_raw)\n",
    "hours_arr = np.array(all_hours)\n",
    "stations_arr = np.array(all_stations)\n",
    "\n",
    "# Number of node-level evaluations after ignoring nodes without targets and nodes without complex mappings\n",
    "print(f\"Total predictions (masked): {len(pred_raw):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60714eb2",
   "metadata": {},
   "source": [
    "## Overall Metrics\n",
    "\n",
    "Calculates and prints:\n",
    "- **MSE/MAE in normalized space**\n",
    "- **MSE/MAE/RMSE in raw space**\n",
    "- **Median absolute error**\n",
    "- **R^2 score**\n",
    "\n",
    "MSE/MAE in the normalised space is measured in z-score units and MSE/MAE/RMSE in the raw space is measured in terms of the real number tap-ins.\n",
    "\n",
    "MedAE is very robust ot outliers. If MAE >> MedAE, this may indicate that there are rare but big misses in the model's predictions.\n",
    "\n",
    "R^2 score measures how much better we are than a dumb baseline like predicting the mean of true_raw. It measures how well the model explains variance.\n",
    "- 1.0 = perfect\n",
    "- 0.0 = no better than predicting the mean\n",
    "- < 0 = worse than predicting the mean \n",
    "\n",
    "This gives a sense of both relative and absolute model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ee68573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "OVERALL METRICS\n",
      "==================================================\n",
      "\n",
      "  Normalized space:\n",
      "    MSE  = 0.1216\n",
      "    MAE  = 0.2011\n",
      "\n",
      "  Raw space:\n",
      "    MSE   = 18685.71\n",
      "    RMSE  = 136.70\n",
      "    MAE   = 48.18 tap-ins\n",
      "    MedAE = 17.73 tap-ins\n",
      "    R^2    = 0.9623\n"
     ]
    }
   ],
   "source": [
    "# --- Normalized space ---\n",
    "mse_norm = np.mean((pred_norm - true_norm) ** 2)\n",
    "mae_norm = np.mean(np.abs(pred_norm - true_norm))\n",
    "\n",
    "# --- Raw space ---\n",
    "mse_raw = np.mean((pred_raw - true_raw) ** 2)\n",
    "mae_raw = np.mean(np.abs(pred_raw - true_raw))\n",
    "rmse_raw = np.sqrt(mse_raw)\n",
    "\n",
    "# --- R² score ---\n",
    "ss_res = np.sum((true_raw - pred_raw) ** 2)\n",
    "ss_tot = np.sum((true_raw - np.mean(true_raw)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "# --- Median absolute error ---\n",
    "median_ae = np.median(np.abs(pred_raw - true_raw))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"OVERALL METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Normalized space:\")\n",
    "print(f\"    MSE  = {mse_norm:.4f}\")\n",
    "print(f\"    MAE  = {mae_norm:.4f}\")\n",
    "print(f\"\\n  Raw space:\")\n",
    "print(f\"    MSE   = {mse_raw:.2f}\")\n",
    "print(f\"    RMSE  = {rmse_raw:.2f}\")\n",
    "print(f\"    MAE   = {mae_raw:.2f} tap-ins\")\n",
    "print(f\"    MedAE = {median_ae:.2f} tap-ins\")\n",
    "print(f\"    R^2    = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2f918",
   "metadata": {},
   "source": [
    "## Error by Hour of Day\n",
    "\n",
    "Shows how model error varies by time of day. This helps us to identify problems like whether or not hte model struggles to predict rush hour traffic correctly.\n",
    "\n",
    "For each hour, prints the MAE and a bar chart for quick visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00a85dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hour       MAE     Count  Bar\n",
      "---------------------------------------------\n",
      "  00:00     21.65    149280  ████\n",
      "  01:00     18.88    146720  ███\n",
      "  02:00     22.57    145604  ████\n",
      "  03:00     16.39    147719  ███\n",
      "  04:00     18.00    151917  ███\n",
      "  05:00     31.06    152980  ██████\n",
      "  06:00     46.81    153202  █████████\n",
      "  07:00     88.29    153251  █████████████████\n",
      "  08:00    117.45    153164  ███████████████████████\n",
      "  09:00     51.13    153139  ██████████\n",
      "  10:00     43.50    153117  ████████\n",
      "  11:00     38.76    153082  ███████\n",
      "  12:00     39.73    153107  ███████\n",
      "  13:00     42.82    153168  ████████\n",
      "  14:00     51.25    153247  ██████████\n",
      "  15:00     64.43    153254  ████████████\n",
      "  16:00     67.14    153210  █████████████\n",
      "  17:00    117.54    153154  ███████████████████████\n",
      "  18:00     59.61    153140  ███████████\n",
      "  19:00     42.46    153044  ████████\n",
      "  20:00     37.03    152967  ███████\n",
      "  21:00     38.79    152888  ███████\n",
      "  22:00     39.48    152579  ███████\n",
      "  23:00     36.80    151862  ███████\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Hour':>6}  {'MAE':>8}  {'Count':>8}  Bar\")\n",
    "print(\"-\" * 45)\n",
    "for h in range(24):\n",
    "    mask = hours_arr == h\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    h_mae = np.mean(np.abs(pred_raw[mask] - true_raw[mask]))\n",
    "    h_count = mask.sum()\n",
    "    bar = \"█\" * int(h_mae / 5)\n",
    "    print(f\"  {h:02d}:00  {h_mae:>8.2f}  {h_count:>8}  {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d597f5c",
   "metadata": {},
   "source": [
    "## Per-Station Breakdown\n",
    "\n",
    "For each station, computes:\n",
    "- MAE\n",
    "- Average true ridership\n",
    "- MAPE (mean absolute percentage error)\n",
    "- Number of predictions\n",
    "\n",
    "Prints the 10 worst and 10 best stations by MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c719b057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 WORST stations (highest MAE):\n",
      "     Station       MAE   Avg Ridership    MAPE%       n\n",
      "  ----------  --------  --------------  -------  ------\n",
      "         611    617.97         5231.56    11.8%    8781\n",
      "         610    542.37         3869.82    14.0%    8781\n",
      "         607    346.41         2855.46    12.1%    8781\n",
      "         628    302.45         2198.59    13.8%    8781\n",
      "         164    287.34         2126.29    13.5%    8781\n",
      "         602    283.82         2613.00    10.9%    8781\n",
      "         318    271.00         1896.79    14.3%    8781\n",
      "         225    242.33         1425.44    17.0%    8781\n",
      "         624    214.74         1525.03    14.1%    8781\n",
      "         614    212.01         1935.46    11.0%    8781\n",
      "Top 10 BEST stations (lowest MAE):\n",
      "     Station       MAE   Avg Ridership    MAPE%       n\n",
      "  ----------  --------  --------------  -------  ------\n",
      "         205      7.27           39.37    18.5%    8604\n",
      "         200      6.93           22.36    31.0%    8295\n",
      "         196      6.29           20.95    30.0%    8457\n",
      "         197      5.16           21.18    24.3%    8276\n",
      "         207      5.11           20.23    25.3%    8118\n",
      "         203      4.94           19.89    24.9%    8590\n",
      "         202      4.88           10.23    47.8%    7188\n",
      "         206      4.80           18.10    26.5%    8469\n",
      "         201      4.73           13.01    36.4%    7917\n",
      "         199      3.11            8.21    37.9%    7717\n"
     ]
    }
   ],
   "source": [
    "station_errors = {}\n",
    "for cmplx_id in np.unique(stations_arr):\n",
    "    mask = stations_arr == cmplx_id\n",
    "    if mask.sum() < 5:\n",
    "        continue\n",
    "    s_mae = np.mean(np.abs(pred_raw[mask] - true_raw[mask]))\n",
    "    s_avg_ridership = np.mean(true_raw[mask])\n",
    "    station_errors[cmplx_id] = {\n",
    "        \"mae\": s_mae,\n",
    "        \"avg_ridership\": s_avg_ridership,\n",
    "        \"mape\": (s_mae / (s_avg_ridership + 1e-6)) * 100,\n",
    "        \"n\": int(mask.sum()),\n",
    "    }\n",
    "\n",
    "sorted_stations = sorted(station_errors.items(), key=lambda x: x[1][\"mae\"], reverse=True)\n",
    "\n",
    "print(\"Top 10 WORST stations (highest MAE):\")\n",
    "print(f\"  {'Station':>10}  {'MAE':>8}  {'Avg Ridership':>14}  {'MAPE%':>7}  {'n':>6}\")\n",
    "print(f\"  {'-'*10}  {'-'*8}  {'-'*14}  {'-'*7}  {'-'*6}\")\n",
    "for cmplx_id, err in sorted_stations[:10]:\n",
    "    print(f\"  {cmplx_id:>10}  {err['mae']:>8.2f}  {err['avg_ridership']:>14.2f}  {err['mape']:>6.1f}%  {err['n']:>6}\")\n",
    "\n",
    "print(\"Top 10 BEST stations (lowest MAE):\")\n",
    "print(f\"  {'Station':>10}  {'MAE':>8}  {'Avg Ridership':>14}  {'MAPE%':>7}  {'n':>6}\")\n",
    "print(f\"  {'-'*10}  {'-'*8}  {'-'*14}  {'-'*7}  {'-'*6}\")\n",
    "for cmplx_id, err in sorted_stations[-10:]:\n",
    "    print(f\"  {cmplx_id:>10}  {err['mae']:>8.2f}  {err['avg_ridership']:>14.2f}  {err['mape']:>6.1f}%  {err['n']:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3f17084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 WORST stations (highest MAE):\n",
      "     Station       MAE   Avg Ridership    MAPE%       n\n",
      "  ----------  --------  --------------  -------  ------\n",
      "         611    617.97         5231.56    11.8%    8781\n",
      "         610    542.37         3869.82    14.0%    8781\n",
      "         607    346.41         2855.46    12.1%    8781\n",
      "         628    302.45         2198.59    13.8%    8781\n",
      "         164    287.34         2126.29    13.5%    8781\n",
      "         602    283.82         2613.00    10.9%    8781\n",
      "         318    271.00         1896.79    14.3%    8781\n",
      "         225    242.33         1425.44    17.0%    8781\n",
      "         624    214.74         1525.03    14.1%    8781\n",
      "         614    212.01         1935.46    11.0%    8781\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 WORST stations (highest MAE):\")\n",
    "print(f\"  {'Station':>10}  {'MAE':>8}  {'Avg Ridership':>14}  {'MAPE%':>7}  {'n':>6}\")\n",
    "print(f\"  {'-'*10}  {'-'*8}  {'-'*14}  {'-'*7}  {'-'*6}\")\n",
    "for cmplx_id, err in sorted_stations[:10]:\n",
    "    print(f\"  {cmplx_id:>10}  {err['mae']:>8.2f}  {err['avg_ridership']:>14.2f}  {err['mape']:>6.1f}%  {err['n']:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4cf6357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 BEST stations (lowest MAE):\n",
      "     Station       MAE   Avg Ridership    MAPE%       n\n",
      "  ----------  --------  --------------  -------  ------\n",
      "         205      7.27           39.37    18.5%    8604\n",
      "         200      6.93           22.36    31.0%    8295\n",
      "         196      6.29           20.95    30.0%    8457\n",
      "         197      5.16           21.18    24.3%    8276\n",
      "         207      5.11           20.23    25.3%    8118\n",
      "         203      4.94           19.89    24.9%    8590\n",
      "         202      4.88           10.23    47.8%    7188\n",
      "         206      4.80           18.10    26.5%    8469\n",
      "         201      4.73           13.01    36.4%    7917\n",
      "         199      3.11            8.21    37.9%    7717\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 BEST stations (lowest MAE):\")\n",
    "print(f\"  {'Station':>10}  {'MAE':>8}  {'Avg Ridership':>14}  {'MAPE%':>7}  {'n':>6}\")\n",
    "print(f\"  {'-'*10}  {'-'*8}  {'-'*14}  {'-'*7}  {'-'*6}\")\n",
    "for cmplx_id, err in sorted_stations[-10:]:\n",
    "    print(f\"  {cmplx_id:>10}  {err['mae']:>8.2f}  {err['avg_ridership']:>14.2f}  {err['mape']:>6.1f}%  {err['n']:>6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360f996",
   "metadata": {},
   "source": [
    "## MAPE Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a8f8911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE across stations:\n",
      "  Median = 15.5%\n",
      "  Mean   = 16.6%\n",
      "  25th   = 13.9%\n",
      "  75th   = 17.7%\n"
     ]
    }
   ],
   "source": [
    "mapes = [v[\"mape\"] for v in station_errors.values()]\n",
    "print(f\"MAPE across stations:\")\n",
    "print(f\"  Median = {np.median(mapes):.1f}%\")\n",
    "print(f\"  Mean   = {np.mean(mapes):.1f}%\")\n",
    "print(f\"  25th   = {np.percentile(mapes, 25):.1f}%\")\n",
    "print(f\"  75th   = {np.percentile(mapes, 75):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c784eb",
   "metadata": {},
   "source": [
    "## MAPE Distribution\n",
    "\n",
    "Shows the distribution of mean absolute percentage error (MAPE) across all stations. Useful for understanding typical vs. worst-case error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e62c0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE across stations:\n",
      "  Median = 15.5%\n",
      "  Mean   = 16.6%\n",
      "  25th   = 13.9%\n",
      "  75th   = 17.7%\n"
     ]
    }
   ],
   "source": [
    "mapes = [v[\"mape\"] for v in station_errors.values()]\n",
    "print(f\"MAPE across stations:\")\n",
    "print(f\"  Median = {np.median(mapes):.1f}%\")\n",
    "print(f\"  Mean   = {np.mean(mapes):.1f}%\")\n",
    "print(f\"  25th   = {np.percentile(mapes, 25):.1f}%\")\n",
    "print(f\"  75th   = {np.percentile(mapes, 75):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "174d0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall (masked) metrics ===\n",
      "MAE   = 48.18\n",
      "RMSE  = 136.70\n",
      "WMAPE = 14.58%\n",
      "\n",
      "=== Baseline: station mean ===\n",
      "MAE   = 235.27\n",
      "WMAPE = 71.18%\n",
      "\n",
      "=== Improvement vs baseline (station mean) ===\n",
      "ΔMAE   = 187.09 (positive means model is better)\n",
      "ΔWMAPE = 56.60 pp (positive means model is better)\n",
      "\n",
      "Top 10 WORST stations by WMAPE%:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>MAE</th>\n",
       "      <th>AvgRidership</th>\n",
       "      <th>n</th>\n",
       "      <th>WMAPE%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>448</td>\n",
       "      <td>152.334252</td>\n",
       "      <td>230.366196</td>\n",
       "      <td>8774</td>\n",
       "      <td>66.126999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>202</td>\n",
       "      <td>4.884463</td>\n",
       "      <td>10.226349</td>\n",
       "      <td>7188</td>\n",
       "      <td>47.763509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>85</td>\n",
       "      <td>26.071011</td>\n",
       "      <td>58.726673</td>\n",
       "      <td>7727</td>\n",
       "      <td>44.393816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>83</td>\n",
       "      <td>30.791106</td>\n",
       "      <td>71.428374</td>\n",
       "      <td>7979</td>\n",
       "      <td>43.107667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>199</td>\n",
       "      <td>3.110756</td>\n",
       "      <td>8.212518</td>\n",
       "      <td>7717</td>\n",
       "      <td>37.878224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>201</td>\n",
       "      <td>4.728581</td>\n",
       "      <td>13.006063</td>\n",
       "      <td>7917</td>\n",
       "      <td>36.356741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>86</td>\n",
       "      <td>12.441631</td>\n",
       "      <td>36.626654</td>\n",
       "      <td>7556</td>\n",
       "      <td>33.968789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>374</td>\n",
       "      <td>10.147177</td>\n",
       "      <td>32.294203</td>\n",
       "      <td>8314</td>\n",
       "      <td>31.421049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>200</td>\n",
       "      <td>6.928390</td>\n",
       "      <td>22.361905</td>\n",
       "      <td>8295</td>\n",
       "      <td>30.983003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>138</td>\n",
       "      <td>19.652060</td>\n",
       "      <td>63.511918</td>\n",
       "      <td>8726</td>\n",
       "      <td>30.942318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Station         MAE  AvgRidership     n     WMAPE%\n",
       "373      448  152.334252    230.366196  8774  66.126999\n",
       "161      202    4.884463     10.226349  7188  47.763509\n",
       "69        85   26.071011     58.726673  7727  44.393816\n",
       "67        83   30.791106     71.428374  7979  43.107667\n",
       "158      199    3.110756      8.212518  7717  37.878224\n",
       "160      201    4.728581     13.006063  7917  36.356741\n",
       "70        86   12.441631     36.626654  7556  33.968789\n",
       "311      374   10.147177     32.294203  8314  31.421049\n",
       "159      200    6.928390     22.361905  8295  30.983003\n",
       "110      138   19.652060     63.511918  8726  30.942318"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 BEST stations by WMAPE%:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>MAE</th>\n",
       "      <th>AvgRidership</th>\n",
       "      <th>n</th>\n",
       "      <th>WMAPE%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>264</td>\n",
       "      <td>48.766114</td>\n",
       "      <td>483.946134</td>\n",
       "      <td>8781</td>\n",
       "      <td>10.076765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>623</td>\n",
       "      <td>127.019270</td>\n",
       "      <td>1258.651976</td>\n",
       "      <td>8781</td>\n",
       "      <td>10.091691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>447</td>\n",
       "      <td>173.204917</td>\n",
       "      <td>1684.875271</td>\n",
       "      <td>8755</td>\n",
       "      <td>10.279985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>263</td>\n",
       "      <td>36.911824</td>\n",
       "      <td>347.711731</td>\n",
       "      <td>8780</td>\n",
       "      <td>10.615639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>602</td>\n",
       "      <td>283.820120</td>\n",
       "      <td>2613.003303</td>\n",
       "      <td>8781</td>\n",
       "      <td>10.861835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>613</td>\n",
       "      <td>148.439435</td>\n",
       "      <td>1365.809817</td>\n",
       "      <td>8781</td>\n",
       "      <td>10.868236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>601</td>\n",
       "      <td>131.112705</td>\n",
       "      <td>1203.958433</td>\n",
       "      <td>8781</td>\n",
       "      <td>10.890136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>451</td>\n",
       "      <td>81.280070</td>\n",
       "      <td>743.927571</td>\n",
       "      <td>8781</td>\n",
       "      <td>10.925804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>614</td>\n",
       "      <td>212.009474</td>\n",
       "      <td>1935.461793</td>\n",
       "      <td>8781</td>\n",
       "      <td>10.953948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>303</td>\n",
       "      <td>29.050506</td>\n",
       "      <td>264.061714</td>\n",
       "      <td>8507</td>\n",
       "      <td>11.001408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Station         MAE  AvgRidership     n     WMAPE%\n",
       "216      264   48.766114    483.946134  8781  10.076765\n",
       "414      623  127.019270   1258.651976  8781  10.091691\n",
       "372      447  173.204917   1684.875271  8755  10.279985\n",
       "215      263   36.911824    347.711731  8780  10.615639\n",
       "393      602  283.820120   2613.003303  8781  10.861835\n",
       "404      613  148.439435   1365.809817  8781  10.868236\n",
       "392      601  131.112705   1203.958433  8781  10.890136\n",
       "376      451   81.280070    743.927571  8781  10.925804\n",
       "405      614  212.009474   1935.461793  8781  10.953948\n",
       "249      303   29.050506    264.061714  8507  11.001408"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- EDIT THESE NAMES ONLY IF YOUR NOTEBOOK USES DIFFERENT ONES ---\n",
    "y_true = true_raw\n",
    "y_pred = pred_raw\n",
    "station_ids = stations_arr\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Safety: drop NaNs/infs if any\n",
    "mask_ok = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "y_true = y_true[mask_ok]\n",
    "y_pred = y_pred[mask_ok]\n",
    "station_ids = station_ids[mask_ok]\n",
    "\n",
    "# ---- Overall metrics (system-wide) ----\n",
    "mae = np.mean(np.abs(y_true - y_pred))\n",
    "rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "wmape = np.sum(np.abs(y_true - y_pred)) / max(np.sum(np.abs(y_true)), 1e-8)   # weighted MAPE\n",
    "\n",
    "print(\"\\n=== Overall (masked) metrics ===\")\n",
    "print(f\"MAE   = {mae:,.2f}\")\n",
    "print(f\"RMSE  = {rmse:,.2f}\")\n",
    "print(f\"WMAPE = {wmape*100:.2f}%\")\n",
    "\n",
    "# ---- Baseline: predict station mean ----\n",
    "# Build baseline predictions aligned with each (station, time) point\n",
    "baseline = np.array([stn_mean.get(int(s), np.nan) for s in station_ids], dtype=float)\n",
    "\n",
    "mask_b = np.isfinite(baseline)\n",
    "y_true_b = y_true[mask_b]\n",
    "y_pred_b = baseline[mask_b]\n",
    "\n",
    "mae_b = np.mean(np.abs(y_true_b - y_pred_b))\n",
    "wmape_b = np.sum(np.abs(y_true_b - y_pred_b)) / max(np.sum(np.abs(y_true_b)), 1e-8)\n",
    "\n",
    "print(\"\\n=== Baseline: station mean ===\")\n",
    "print(f\"MAE   = {mae_b:,.2f}\")\n",
    "print(f\"WMAPE = {wmape_b*100:.2f}%\")\n",
    "\n",
    "print(\"\\n=== Improvement vs baseline (station mean) ===\")\n",
    "print(f\"ΔMAE   = {mae_b - mae:,.2f} (positive means model is better)\")\n",
    "print(f\"ΔWMAPE = {(wmape_b - wmape)*100:.2f} pp (positive means model is better)\")\n",
    "\n",
    "# ---- Optional: per-station WMAPE leaderboard (fairer than MAE-only) ----\n",
    "df_cmp = pd.DataFrame({\n",
    "    \"Station\": station_ids.astype(int),\n",
    "    \"y_true\": y_true,\n",
    "    \"y_pred\": y_pred,\n",
    "})\n",
    "df_cmp[\"abs_err\"] = (df_cmp[\"y_true\"] - df_cmp[\"y_pred\"]).abs()\n",
    "\n",
    "per_station_wmape = (\n",
    "    df_cmp.groupby(\"Station\")\n",
    "    .agg(\n",
    "        WMAPE=(\"abs_err\", lambda s: s.sum() / max(df_cmp.loc[s.index, \"y_true\"].abs().sum(), 1e-8)),\n",
    "        MAE=(\"abs_err\", \"mean\"),\n",
    "        AvgRidership=(\"y_true\", \"mean\"),\n",
    "        n=(\"y_true\", \"size\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "per_station_wmape[\"WMAPE%\"] = per_station_wmape[\"WMAPE\"] * 100\n",
    "per_station_wmape = per_station_wmape.drop(columns=[\"WMAPE\"])\n",
    "\n",
    "print(\"\\nTop 10 WORST stations by WMAPE%:\")\n",
    "display(per_station_wmape.sort_values(\"WMAPE%\", ascending=False).head(10))\n",
    "\n",
    "print(\"\\nTop 10 BEST stations by WMAPE%:\")\n",
    "display(per_station_wmape.sort_values(\"WMAPE%\", ascending=True).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
